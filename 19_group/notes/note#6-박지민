# 기계학습 - 정리노트 #6

### 앙상블 학습

`앙상블 학습`이란 여러 개의 분류기를 생성하고  
그 예측을 결합하여 보다 정확한 예측을 도출하는 기법

#### 앙상블 학습의 동작 원리

1. 여러 개의 분류기 생성
2. 생성된 분류기들의 예측 결합
3. 결합된 예측 성능 평가

### 랜덤 포레스트

`랜덤 포레스트`란 배깅과 페이스팅 방법을 적용한 결정트리의 앙상블을 최적화한 모델

### 투표 기반 분류기

- `투표 기반 분류기` : 여러 분류기를 이용하여 앙상블 학습 적용 후 투표로 예측값 결정
  - `직접 투표` : 다수결 투표
    - 장점 : 확률 계산이 필요 없으므로 계산 비용이 줄어듬.
    - 단점 : 각 분류기의 예측에 대한 **신뢰도**를 고려하기 않기에 신뢰도가 낮은 분류기의 예측에도 과대적합될 가능성이 있음.
  - `간접 투표` : 각 분류기의 예측값의 평균 확률이 가장 높은 클래스를 예측값으로 결정
    - 장점 : 각 분류기의 예측에 대한 **신뢰도**를 고려함.
    - 단점 : 확률 계산을 하므로 간접 투표는 직접 투표보다 더 많은 계산 비용이 발생.

### 배깅과 페이스팅

- `배깅` : 중복 허용 샘플링 방식
- `페이스팅` : 중복을 미허용 샘플링 방식

### oob 평가

- `oob(out-of-bag) 샘플` : 배깅 방식으로 훈련된 분류기에 포함되지 않은 샘플
- `oob(out-of-bag) 평가` : oob 샘플을 이용하여 평가하는 방식

```python
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    bootstrap=True, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_
```

위 코드는 `oob 평가`를 이용하여 `배깅 분류기`를 훈련시키고 평가하는 코드임.

### 에이다 부스트

`에이다 부스트`란 이전 분류기의 예측이 잘못된 샘플에  
**가중치**를 높여서 다음 분류기를 훈련시키는 방식

#### 에이다 부스트의 동작원리

1. 훈련 세트에서 중복을 허용하지 않고 무작위로 샘플링하여 크기가 m인 서브셋을 만듬.
2. 이전에 만들어진 분류기들을 이용하여 서브셋을 이용하여 훈련시킴.
3. 가중치가 적용된 에러율을 이용하여 예측기의 가중치를 계산함.
4. 예측기의 가중치가 적용된 에러율을 이용하여 예측기의 가중치를 계산함.

### 그레이디언트 부스팅

`그레이디언트 부스팅`은 이전 학습기의 오차를 보정하는 방식으로 새로운 예측기를 순차적으로 추가하는 방식
